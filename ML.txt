supervised
	Regression
		linear regression
			in stat is the process of predicting the label based on the independent variable 
				
		decision tree regression
			deep overfitted model - to tackle this we can use pruning, Gini and entropy (the lesser the entropy the higher the information gain)
			https://www.geeksforgeeks.org/pros-and-cons-of-decision-tree-regression-in-machine-learning/
			https://medium.com/analytics-vidhya/decision-trees-for-classification-id3-machine-learning-6844f026bf1a
			https://towardsdatascience.com/machine-learning-basics-decision-tree-regression-1d73ea003fda
			https://towardsdatascience.com/decision-trees-explained-entropy-information-gain-gini-index-ccp-pruning-4d78070db36c
			https://towardsdatascience.com/decision-trees-introduction-intuition-dac9592f4b7f#:~:text=As%20shown%20in%20the%20above,initial%20splitting%			20of%20data%20records.
		
		SVM

		Random forest(bagging- reducing the variance in a model without impacting the bias, it is called as bootstrap aggregation)
			Random forest = DT(base model) + bagging(row sampling with replacement) + column sampling(features)	
			bootstrap = sampling with replacement
			aggregation = aggregating the model
			(https://towardsdatascience.com/machine-learning-basics-random-forest-regression-be3e1e3bb91a)
			(https://www.geeksforgeeks.org/random-forest-algorithm-in-machine-learning/)

		Gradient boosting(boosting)
			GBDT = pseudo residual + Additive(lr like this)
			it consist of shallow decision tree where high bias low variance
			Pseudo residual, regularization, shrinkage using these three technique reduce bias
			Pseudo residual - intermediate error rate
			shrinkage - learning rate(range from 0-1)
			(https://www.geeksforgeeks.org/ml-gradient-boosting/)
			(https://machinelearningmastery.com/gentle-introduction-gradient-boosting-algorithm-machine-learning/)
			(https://machinelearningmastery.com/gradient-boosting-machine-ensemble-in-python/)

		XG boost regression(boosting)
			Extra gradient boosting 
			XG boost = GBDT + row sampling + column sampling 
			(https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)
			
		Ada boosting(boosting) - this is for good image
			
		Stacking - This is called MLxtend classifier
		
		Cascading - 
		
		Bagging involves fitting many decision trees on different samples of the same dataset and averaging the predictions.
		Stacking involves fitting many different models types on the same data and using another model to learn how to best combine the predictions.
		Boosting involves adding ensemble members sequentially that correct the predictions made by prior models and outputs a weighted average of the 			predictions.
				
	classification

		Logistic regression
			here is plane because 2d space
			Classes are slaced perfectely linearly separable where pi = w^t * x + b 
			(https://www.geeksforgeeks.org/understanding-logistic-regression/)
			(https://medium.com/analytics-vidhya/logistic-regression-geometric-interpretation-80942d0286b6)
			

		KNN
			(https://www.ibm.com/topics/knn#:~:text=The%20k%2Dnearest%20neighbors%20(KNN,used%20in%20machine%20learning%20today.)
			(https://medium.com/@vagadro/knn-algorithm-a-geometric-intuition-c54cfbb3ef98)
			KD tree 
			(https://medium.com/analytics-vidhya/understanding-k-nearest-neighbour-algorithm-in-detail-fc9649c1d196)
			(https://medium.com/@vagadro/knn-algorithm-a-geometric-intuition-c54cfbb3ef98)
			(https://towardsdatascience.com/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761)
			(https://www.ibm.com/topics/knn#:~:text=The%20k-nearest%20neighbors%20(KNN,used%20in%20machine%20learning%20today.)
			(https://www.geeksforgeeks.org/k-nearest-neighbours/)
			geometric intuition of KNN
	
		SVM
			Hyperplane because nd space.
			Classes are slaced perfectely linearly separable 
			Teh distance betn hyperplane and supporvector called hard margin and it is linearly separable which is rare to see			
			Teh distance betn hyperplane and supporvector called soft marging and it is not linearly separable 
			(https://www.geeksforgeeks.org/major-kernel-functions-in-support-vector-machine-svm/)
			(https://www.geeksforgeeks.org/support-vector-machine-algorithm/)
			(https://towardsdatascience.com/demystifying-support-vector-machines-8453b39f7368#:~:text=and%20so%20on.-,Geometric%20Intuition,the%20negative%			20plane%20is%20maximum.)
			(https://towardsdatascience.com/machine-learning-basics-support-vector-regression-660306ac5226)

		decision tree regression
		Random forest
		Gradient boosting
		XG boost regression

unsupervised
	Clustering 
		it is a collection of similar data points, groups of similar objects, To find a structure in a collection of unlabeled data types, it 				identifies homogeneous subgroup among the observation. 
		1. K-mean, 2. Hierarchal clustering, 3. DB-scan

	k-mean (centroid) 
		(https://towardsdatascience.com/k-means-clustering-for-beginners-2dc7b2994a4)
		(https://www.geeksforgeeks.org/k-means-clustering-introduction/)
		(https://towardsdatascience.com/explain-ml-in-a-simple-way-k-means-clustering-e925d019743b#:~:text=Intra-cluster%20variance%20(a.k.a.%2C,used%20to%		20quantify%20internal%20cohesion.)
		(https://towardsdatascience.com/three-versions-of-k-means-cf939b65f4ea)
		it is a centroid base clustering. First we randomly declare k. Now 
		
		K-mean++
			(https://towardsdatascience.com/understanding-k-means-k-means-and-k-medoids-clustering-algorithms-ad9c9fbf47ca)

		k-medoid
			(https://towardsdatascience.com/understanding-k-means-k-means-and-k-medoids-clustering-algorithms-ad9c9fbf47ca)

		lloyds
			(https://towardsdatascience.com/three-versions-of-k-means-cf939b65f4ea)

	Hierarchal clustering- (https://towardsdatascience.com/understanding-the-concept-of-hierarchical-clustering-technique-c6e8243758ec)
		(https://www.geeksforgeeks.org/hierarchical-clustering-in-data-mining/)
		(https://www.geeksforgeeks.org/hierarchical-clustering/)
		(https://www.geeksforgeeks.org/difference-between-k-means-and-hierarchical-clustering/)
		1. Agglomerative
		2. Divisive
		
	Density-Based 
		(https://towardsdatascience.com/dbscan-make-density-based-clusters-by-hand-2689dc335120)
		(https://www.geeksforgeeks.org/dbscan-clustering-in-ml-density-based-clustering/)

	What is the difference betn k-mean and KNN
	
reinforcement

Ensembel learning 
	(https://towardsdatascience.com/practical-guide-to-ensemble-learning-d34c74e022a0)


Q :- What is regularization ?
	(https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a)
	This is used to reduce the variance/Overfitting of the model with out impacting the bias. 
	1. Lasso(least absolute srinkage and selection operator) this is also called as L1 regularization
		it srink the less important features coefficient to zero and remove the feature.
	2. Ridge is also called L2 regularization. it srink the less important features coefficient near to zero and it will not remove the feature. 

Q :- R^2 and adjusted R^2 ?
	(https://medium.com/analytics-vidhya/r-squared-and-adjusted-r-squared-408aaca84fd5#:~:text=The%20only%20difference%20between%20them,variation%20in%20the%	20dependent%20variable.&text=Adjusted%20R%2DSquared-,where%2C,the%20number%20of%20data%20points.)
	R^2 it is a statical measure of how close the data R to the fitted regression line. The range is 0 to 1. R^2 = 
	Adjusted R^2 There is a problem in R^2 that if we increase independent variable it increase the R^2 with out any correlation. In adjusted R^2 if we increase 	the independent variable if there is no corelation with the output variable it will penalize the independent variable.

Q :- Performance factor of regression model ?
	(https://towardsdatascience.com/assessing-model-performance-for-regression-7568db6b2da0#:~:text=trackable%20performance%20metrics.-,Performance%20Metrics%	20for%20Regression,Mean%20Squared%20Error%20(RMSE).)
	R^2
	MSE
	RMSE
	MAE
	Residual plots (https://towardsdatascience.com/how-to-use-residual-plots-for-regression-model-validation-c3c70e8ab378)
	Cross validation
	AIC and BIC

Q :- what is Entropy ?
	It is the measure of randomness in the data. It check the impurity present in the dataset. 

Q :- What is information gain ?
	It is the diff betn the entropy of a data segment before the split and after the split.

Q :- What is Gini ?
	To evaluate the impurity or homogenity of a data. If the gini index is high then it indicate that the dataset at particular node in the DT is impure.

Q :- Divide and concure ?
	It is a strategy  to split the dataset.

Q :- How logistic regression work ?
	It is working on the probability based algorithm.

Q :- What is pruning ?
	Pre pruning - Early stopping or forward pruning. The feature which is unnecessery that leads to a complex tree so we need pre prunning.
	Post Pruning - 

Q :- OneHot encoding ?
	It is also called label encoding

Q :- Variance and bias trade off ?
	

Q :- Performance matrix ?
	Confusion matrix
	Accuracy
	Sencitivity
	Specificity
	Recall
	precision 
	F1 score

Q :- What is normalization ?

Q :- Diff feature selection and feature extraction ?
	(https://www.geeksforgeeks.org/difference-between-feature-selection-and-feature-extraction/)
	feature extraction  it involves transformation of features so that we can extract features to improve the process of feature selection.
			
Q :- What are the ML supervised algorithem are used for outlier detection ?
	SVM, K-NN

Q :- What are the ML unsupervised algorithem are used for outlier detection 
	DBSCAN, K-mean

Q :- What is  intra-cluster variance ?

To measure text similarity - (https://medium.com/@ahmetmnirkocaman/how-to-measure-text-similarity-a-comprehensive-guide-6c6f24fc01fe)

PCA :- 

normalize the data
calculate the covariance using matrix
calculate the eigne value and eigne vector
choosing components and forming a feature vector
forming principal component

ARIMA - Auto regresive integrated moving average

P - Lag order
D - Differentiate
Q - Moving average
